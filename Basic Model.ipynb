{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtmoncur/.local/lib/python3.5/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"ggplot\")\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "teams_abbr = {'- - -': '- - -', 'ANA': 'Angels', 'ARI': 'Diamondbacks', \n",
    " 'ATL': 'Braves', 'BAL': 'Orioles', 'BOS': 'Red Sox', 'CHC': 'Cubs', \n",
    " 'CHW': 'White Sox', 'CIN': 'Reds', 'CLE': 'Indians', 'COL': 'Rockies',\n",
    " 'DET': 'Tigers', 'FLA': 'Marlins', 'HOU': 'Astros', 'KCR': 'Royals', \n",
    " 'LAA': 'Angels', 'LAD': 'Dodgers', 'MIA': 'Marlins', 'MIL': 'Brewers',\n",
    " 'MIN': 'Twins', 'MON': 'Expos', 'NYM': 'Mets', 'NYY': 'Yankees',\n",
    " 'OAK': 'Athletics', 'PHI': 'Phillies', 'PIT': 'Pirates', 'SDP': 'Padres',\n",
    " 'SEA': 'Mariners', 'SFG': 'Giants', 'STL': 'Cardinals', 'TBD': 'Devil Rays', \n",
    " 'TBR': 'Rays', 'TEX': 'Rangers', 'TOR': 'Blue Jays', 'WSN': 'Nationals'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in all data\n",
    "sp = pd.read_csv('FanGraphs_Starters (2).csv')\n",
    "rp = pd.read_csv('FanGraphs_Relievers (2).csv')\n",
    "fd = pd.read_csv('FanGraphs_Fielders (2).csv')\n",
    "bt = pd.read_csv('FanGraphs_Batting (2).csv')\n",
    "\n",
    "a, b, c = len(sp),len(rp),len(fd)\n",
    "print(\"\"\"Number of starter pitchers:\\t{}\n",
    "Number of relief pitchers:\\t{}\n",
    "Number of fielders:\\t\\t{}\n",
    "-------------------------------------\n",
    "Total number of records:\\t{}\"\"\".format(a, b, c, a+b+c))\n",
    "\n",
    "#change abbreviation to full team name\n",
    "fd.loc[[True]*len(fd),'Team'] = [teams_abbr[k] for k in fd.Team]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bt.columns)\n",
    "bt = bt[['Season', 'Name', 'Team', 'G', 'playerid']]\n",
    "bt.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rp.columns)\n",
    "rp = rp[['Season', 'Name', 'Team', 'G', 'GS', 'IP', 'playerid']]\n",
    "rp['Inn'] = rp['IP']\n",
    "rp.drop('IP', axis=1, inplace=True)\n",
    "rp.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp.columns\n",
    "sp = sp[['Season', 'Name', 'Team', 'G', 'GS', 'IP', 'playerid']]\n",
    "sp['Inn'] = sp['IP']\n",
    "sp.drop('IP', axis=1, inplace=True)\n",
    "sp.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fd.columns)\n",
    "fd = fd[['Season', 'Name', 'Team', 'Pos', 'Inn', 'playerid']]\n",
    "fd.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a 3 sets containing tuples of playerids with the season for each record\n",
    "fd_season_id = set([tuple(row) for row in fd[['playerid','Season']].values])\n",
    "rp_season_id = set([tuple(row) for row in rp[['playerid','Season']].values])\n",
    "sp_season_id = set([tuple(row) for row in sp[['playerid','Season']].values])\n",
    "bt_season_id = set([tuple(row) for row in bt[['playerid','Season']].values])\n",
    "\n",
    "a, b, c, d = len(sp_season_id), len(rp_season_id), len(fd_season_id), len(bt_season_id)\n",
    "total = set()\n",
    "total.update(sp_season_id);total.update(rp_season_id);total.update(fd_season_id);total.update(bt_season_id)\n",
    "\n",
    "print(\"Only counting players once per season.\\n\")\n",
    "print(\"\"\"Number of starter pitchers:\\t{}\n",
    "Number of relief pitchers:\\t{}\n",
    "Number of fielders:\\t\\t{}\n",
    "Number of batters:\\t\\t{}\n",
    "-------------------------------------\n",
    "Total number of records:\\t{}\"\"\".format(a, b, c, d, len(total)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bt_season_id.intersection(fd_season_id)))\n",
    "print(len(bt_season_id.intersection(rp_season_id)))\n",
    "print(len(bt_season_id.intersection(sp_season_id)))\n",
    "print(len(rp_season_id.intersection(fd_season_id)))\n",
    "print(len(rp_season_id.intersection(sp_season_id)))\n",
    "print(len(sp_season_id.intersection(fd_season_id)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine the pitchers first\n",
    "\n",
    "The three below are the only ones that switched between starter and relief pitcher mid season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "name = 'Scot Shields'\n",
    "print('sp', sp[sp.Name==name], end='\\n')\n",
    "print('rp', rp[rp.Name==name], end='\\n')\n",
    "print('fd', fd[fd.Name==name], end='\\n')\n",
    "print('bt', bt[bt.Name==name], end='\\n\\n')\n",
    "\n",
    "\n",
    "name = 'Tim Wakefield'\n",
    "print('sp', sp[sp.Name==name], end='\\n')\n",
    "print('rp', rp[rp.Name==name], end='\\n')\n",
    "print('fd', fd[fd.Name==name], end='\\n')\n",
    "print('bt', bt[bt.Name==name], end='\\n\\n')\n",
    "\n",
    "name = 'Mike Montgomery'\n",
    "print('sp', sp[sp.Name==name], end='\\n')\n",
    "print('rp', rp[rp.Name==name], end='\\n')\n",
    "print('fd', fd[fd.Name==name], end='\\n')\n",
    "print('bt', bt[bt.Name==name], end='\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = pd.merge(rp, sp, on=['playerid', 'Season'], how='outer')\n",
    "print(temp.Name_x.count())\n",
    "print(temp.Name_y.count())\n",
    "print(temp.Season.count())\n",
    "print(temp[(~temp.Name_x.isnull())*(~temp.Name_y.isnull())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#solve these rare occurrences by hand\n",
    "#Scot spent more time as a relief pitcher\n",
    "sp['Pos'] = 'SP'\n",
    "rp['Pos'] = 'RP'\n",
    "sp.drop( sp[(sp['Name']=='Scot Shields')*(sp['Season']==2003)].index, inplace=True )\n",
    "\n",
    "#Tim spent more time as a starter\n",
    "rp.drop( rp[(rp['Name']=='Tim Wakefield')*(rp['Season']==2002)].index, inplace=True )\n",
    "\n",
    "#Mike spent more time as a starter\n",
    "rp.drop( rp[(sp['Name']=='Mike Montgomery')*(sp['Season']==2017)].index, inplace=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pitchers = pd.concat([sp, rp], ignore_index=True)\n",
    "pitchers.sort_values(by=['playerid', 'Season'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the batters data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#drop any pitchers from the fielders since pitchers may play field positions on occasion\n",
    "pitcher_ids = pitchers['playerid'].unique()\n",
    "pitchers_playing_field = fd['playerid'].isin(pitcher_ids)\n",
    "print(fd[pitchers_playing_field])\n",
    "fd.drop( fd[pitchers_playing_field].index , inplace=True) #there are 63 pitchers in fd that aren't in rp or sp\n",
    "\n",
    "fd.drop( fd[fd['Pos'] == 'P'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#change all fielder positions (except catcher) to FD\n",
    "fd.loc[fd.Pos.isin(['1B','2B','3B','RF','LF','CF','SS']),'Pos'] = 'FD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#merge batters with fielders\n",
    "fielders = pd.merge(bt, fd, on=['Season', 'Name', 'Team', 'playerid'], how='inner')\n",
    "fielders.sort_values(by=['playerid', 'Season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fielders['GS'] = 0\n",
    "p = pd.concat([pitchers, fielders], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Old Code for Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create temporary Seasonid column\n",
    "fd.insert(2,'Seasonid',[tuple(row) for row in fd[['playerid','Season']].values])\n",
    "rp.insert(2,'Seasonid',[tuple(row) for row in rp[['playerid','Season']].values])\n",
    "sp.insert(2,'Seasonid',[tuple(row) for row in sp[['playerid','Season']].values])\n",
    "\n",
    "#find and remove duplicates between dataframes\n",
    "#mask = fd.Seasonid.isin(sp_season_id)\n",
    "#fd.drop(fd[mask].index,inplace=True)\n",
    "\n",
    "#mask = fd.Seasonid.isin(rp_season_id)\n",
    "#fd.drop(fd[mask].index,inplace=True)\n",
    "\n",
    "mask = rp.Seasonid.isin(sp_season_id)\n",
    "rp.drop(rp[mask].index,inplace=True)\n",
    "\n",
    "#remove temporary Seasonid column\n",
    "fd.drop('Seasonid',axis=1,inplace=True)\n",
    "sp.drop('Seasonid',axis=1,inplace=True)\n",
    "rp.drop('Seasonid',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change all fielder positions (except catcher) to FD\n",
    "fd.loc[fd.Pos.isin(['1B','2B','3B','RF','LF','CF','SS']),'Pos'] = 'FD'\n",
    "\n",
    "#add column with potision to prepare for combining data\n",
    "sp.insert(2, 'Pos', 'SP')\n",
    "rp.insert(2,'Pos', 'RP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = pd.concat([sp,rp,fd], ignore_index=True)\n",
    "#p = pd.merge(sp,rp, on=['Season', 'Name']'Seasonid', how='outer')\n",
    "#p = pd.merge(p, fd, on='Seasonid', how='outer')\n",
    "p.drop(p[p.Season<2002].index,inplace=True)\n",
    "p.reset_index()\n",
    "\n",
    "p[['W', 'L']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = p.drop(['rSB', 'rGDP', 'rARM', 'rGFP',\n",
    "       'rPM', 'BIZ', 'Plays', 'RZR', 'OOZ', 'CPP', 'TZL', 'FSR',\n",
    "       'ARM', 'DPR', 'RngR', 'ErrR', 'UZR/150', 'Def',\n",
    "        'L', 'SV', 'G', 'GS', 'IP', 'K/9',\n",
    "       'BB/9', 'HR/9', 'BABIP', 'LOB%', 'GB%', 'HR/FB', 'ERA', 'xFIP',\n",
    "       ], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p['DRS'].fillna(p['DRS'].mean(), inplace=True)\n",
    "p['RPP'].fillna(p['RPP'].mean(), inplace=True)\n",
    "p['UZR'].fillna(p['UZR'].mean(), inplace=True)\n",
    "p['FIP'].fillna(p['FIP'].mean(), inplace=True)\n",
    "p['Inn'].fillna(p['Inn'].mean(), inplace=True)\n",
    "p['WAR'].fillna(p['WAR'].mean(), inplace=True)\n",
    "p['W'].fillna(p['W'].mean(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "by_pos = p.groupby('Pos')\n",
    "for g in by_pos.groups:\n",
    "    group = by_pos.get_group(g)\n",
    "    print('#'*50)\n",
    "    print(g)\n",
    "    for c in p.columns:\n",
    "        print(c,'\\t', len(group)-group[c].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#account for team name changes\n",
    "p.loc[p.Team=='Devil Rays','Team'] = 'Rays'\n",
    "p.loc[p.Team=='Expos','Team'] = 'Nationals'\n",
    "del(teams_abbr['MON'])\n",
    "del(teams_abbr['TBD'])\n",
    "\n",
    "#delete players from a season if their team is unknown\n",
    "p.drop(p[p.Team=='- - -'].index,inplace=True)\n",
    "del(teams_abbr['- - -'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Number of records before:\\t{}\".format(len(p)))\n",
    "p.drop_duplicates(inplace=True)\n",
    "print(\"Number of records after:\\t{}\".format(len(p)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "p.dropna(axis=1, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineer Stay Length, Career Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if 'Leave' in p.columns:\n",
    "    p.drop('Leave', axis=1, inplace=True)\n",
    "if 'Stay_Length' in p.columns:\n",
    "    p.drop('Stay_Length', axis=1, inplace=True)\n",
    "if 'Career_Length' in p.columns:\n",
    "    p.drop('Career_Length', axis=1, inplace=True)\n",
    "    \n",
    "#initialize the new column with value 'No Change'\n",
    "p.insert(1, 'Leave', 0)\n",
    "p.insert(1, 'Stay_Length', 0)\n",
    "p.insert(1, 'Career_Length', 0)\n",
    "by_player = p.groupby('playerid')\n",
    "\n",
    "#loop through, player by player\n",
    "for g in by_player.groups:\n",
    "    one_p = by_player.get_group(g)\n",
    "    \n",
    "    stay_counter = 1\n",
    "    career_counter = 1\n",
    "    #loop through the years for each player\n",
    "    for y in sorted(one_p['Season'].unique()):\n",
    "        p.loc[one_p[one_p.Season==y].index,'Career_Length'] = career_counter\n",
    "        p.loc[one_p[one_p.Season==y].index,'Stay_Length'] = stay_counter\n",
    "        \n",
    "        if sum(one_p.Season==y+1)==0:\n",
    "            #case: no next season\n",
    "            if sum(one_p.Season==y+2)==0:\n",
    "                #Case: no next two seasons, counted as leave\n",
    "                p.loc[one_p[one_p.Season==y].index,'Leave'] = 1\n",
    "                stay_counter = 1\n",
    "                career_counter += 1\n",
    "            elif (one_p[one_p.Season==y].Team.values[0] != \n",
    "                    one_p[one_p.Season==y+2].Team.values[0]):\n",
    "                #case come back to different team after missing a a season\n",
    "                p.loc[one_p[one_p.Season==y].index,'Leave'] = 1\n",
    "                stay_counter = 1\n",
    "                career_counter += 1\n",
    "            else:\n",
    "                #case: come back to same team after missing a season\n",
    "                career_counter += 1\n",
    "                stay_counter += 1 \n",
    "        \n",
    "        #Case: Leave in next season\n",
    "        elif (one_p[one_p.Season==y].Team.values[0] != \n",
    "                    one_p[one_p.Season==y+1].Team.values[0]):\n",
    "            p.loc[one_p[one_p.Season==y].index,'Leave'] = 1\n",
    "            stay_counter = 1\n",
    "            career_counter += 1\n",
    "        else:\n",
    "            #case: stay for next season\n",
    "            career_counter += 1\n",
    "            stay_counter += 1\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p.sort_values(by=['playerid','Season']).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine records of players playing multiple positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1 = []\n",
    "by_player = p.groupby('playerid')\n",
    "\n",
    "#loop through, player by player\n",
    "for g in by_player.groups:\n",
    "    one_p = by_player.get_group(g)\n",
    "    for year in one_p.Season.unique():\n",
    "        df1.append([one_p.Name.values[0],year])\n",
    "        #for team in one_p.Team.unique():\n",
    "        #df1.append([one_p.Team.values[0],year])\n",
    "        \n",
    "df1 = pd.DataFrame(df1, columns=['Name','Year'])\n",
    "df1.insert(0,'Team',0)\n",
    "df1.insert(0,'G',0)\n",
    "df1.insert(0,'playerid',0)\n",
    "df1.insert(0,'Pos',0)\n",
    "df1.insert(0,'Inn',0)\n",
    "df1.insert(0,'GS',0)\n",
    "df1.insert(0,'Stay_Length',0)\n",
    "df1.insert(0,'Career_Length',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in df1.index:\n",
    "    #print(p[ p.index == row]).\n",
    "    row = df1[df1.index==idx]\n",
    "    name = row['Name'].values[0]\n",
    "    season = row['Year'].values[0]\n",
    "    name_mask = (p.Name == name)\n",
    "    season_mask = (p.Season == season)\n",
    "    records = (p[name_mask*season_mask])\n",
    "    I = records.Inn.sum()\n",
    "    \n",
    "    \n",
    "    #print(records)\n",
    "    #G = records.G.sum()\n",
    "    df1.loc[idx,'Inn'] = I\n",
    "    df1.loc[idx,'Team'] = records.Team.values[0] \n",
    "    df1.loc[idx,'Pos'] = records.Pos.values[0]\n",
    "    df1.loc[idx,'playerid'] = records.playerid.values[0]\n",
    "    df1.loc[idx,'G'] = records.G.values[0]\n",
    "    df1.loc[idx,'GS'] = records.GS.values[0]\n",
    "    df1.loc[idx,'Stay_Length'] = records.Stay_Length.values[0]\n",
    "    df1.loc[idx,'Career_Length'] = records.Career_Length.values[0]\n",
    "    \n",
    "print(df1.head())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.sort_values(by=['playerid', 'Year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['P', 'SP', 'RP', 'FD', 'C']:\n",
    "    if col in p.columns:\n",
    "        p.drop(col, axis=1, inplace=True)\n",
    "\n",
    "#one-hot encode positions\n",
    "for position in p.Pos.unique():\n",
    "    p.insert(0, position, (p.Pos==position).apply(int))\n",
    "\n",
    "#one-hot encode teams\n",
    "for team in p.Team.unique():\n",
    "    p.insert(0, team, (p.Team==team).apply(int))\n",
    "\n",
    "p.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = p[list(p.Pos.unique())+['Stay_Length', 'Career_Length', 'GS', 'G', 'Inn']]#list(p.Team.unique())+\n",
    "#temp = p[['Stay_Length', 'FD', 'C', 'P', 'SP', 'RP', 'Inn', 'W', 'WAR', 'DRS', 'RPP', 'UZR', 'FIP']]\n",
    "#temp['P*WAR'] = temp['P']*temp['WAR']\n",
    "#temp['P*FIP'] = temp['P']*temp['FIP']\n",
    "#temp['FD*UZR'] = temp['FD']*temp['UZR']\n",
    "#temp['C*DRS'] = temp['C']*temp['DRS']\n",
    "\n",
    "#X = temp[['Stay_Length', 'FD', 'C', 'P', 'RP', 'SP']]#, 'Inn', 'W', 'RPP', 'P*WAR', 'C*DRS', 'FD*UZR', 'P*FIP']]\n",
    "X = temp\n",
    "\n",
    "y = p['Leave']\n",
    "\n",
    "pd.concat([X,y],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = sm.OLS(y,X).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.average(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = results.predict(X)\n",
    "\n",
    "plt.hist(temp.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = results.predict(X[X.RP==1])\n",
    "plt.hist(temp.values)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(X[(y==1)&(X.RP==1)].Stay_Length)\n",
    "plt.show()\n",
    "\n",
    "np.average(X[(y==1)&(X.RP==1)].Stay_Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = results.predict(X[X.SP==1])\n",
    "plt.hist(temp.values)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(X[(y==1)&(X.SP==1)].Stay_Length)\n",
    "plt.show()\n",
    "\n",
    "np.average(X[(y==1)&(X.SP==1)].Stay_Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = results.predict(X[X.FD==1])\n",
    "plt.hist(temp.values)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(X[(y==1)&(X.FD==1)].Stay_Length)\n",
    "plt.show()\n",
    "\n",
    "np.median(X[(y==1)&(X.FD==1)].Stay_Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = results.predict(X[X.C==1])\n",
    "plt.hist(temp.values)\n",
    "plt.show()\n",
    "\n",
    "plt.hist(X[(y==1)&(X.C==1)].Stay_Length)\n",
    "plt.show()\n",
    "\n",
    "np.average(X[(y==1)&(X.C==1)].Stay_Length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for col in ['P', 'RP','SP','C','FD']:\n",
    "#    print(col)\n",
    "data = [X[(y==1)&(X[col]==1)].Stay_Length.values for col in ['RP','SP','C','FD']]\n",
    "plt.boxplot(data)\n",
    "plt.xticks(np.arange(1,1+5), ['RP','SP','C','FD'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X[(y==1)&(X[col]==1)].Stay_Length.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "# Choose some parameter combinations to try\n",
    "parameters = {'n_estimators': [9, 25, 40],\n",
    "              'max_features': ['log2', 'sqrt','auto'], \n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [10, 3, 5, 15], \n",
    "              'min_samples_split': [3, 5, 8],\n",
    "              'min_samples_leaf': [5, 1,8]\n",
    "             }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(rf, parameters, n_jobs=4)#, scoring=acc_scorer)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "rf = grid_obj.best_estimator_\n",
    "\n",
    "\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argsort(rf.feature_importances_)\n",
    "plt.figure(figsize=(10,12))\n",
    "\n",
    "plt.barh(range(len(rf.feature_importances_)),rf.feature_importances_[ind])\n",
    "# It is very important that you use the same columns that you fit your model with, or else this will be wrong!\n",
    "plt.yticks(range(len(rf.feature_importances_)),X_train.columns[ind])\n",
    "plt.title(\"Variable Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "et = ExtraTreesClassifier()\n",
    "\n",
    "# Choose some parameter combinations to try\n",
    "parameters = {'n_estimators': [9, 4, 6],\n",
    "              'max_features': ['log2', 'sqrt','auto'], \n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [10, 2, 3, 5], \n",
    "              'min_samples_split': [2, 3, 5],\n",
    "              'min_samples_leaf': [5, 1,8]\n",
    "             }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(et, parameters, n_jobs=4)#, scoring=acc_scorer)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "et = grid_obj.best_estimator_\n",
    "\n",
    "\n",
    "et.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, et.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "et.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.argsort(et.feature_importances_)\n",
    "plt.figure(figsize=(10,12))\n",
    "\n",
    "plt.barh(range(len(et.feature_importances_)),et.feature_importances_[ind])\n",
    "# It is very important that you use the same columns that you fit your model with, or else this will be wrong!\n",
    "plt.yticks(range(len(et.feature_importances_)),X_train.columns[ind])\n",
    "plt.title(\"Variable Importance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "ab = ExtraTreesClassifier()\n",
    "\n",
    "# Choose some parameter combinations to try\n",
    "parameters = {'n_estimators': [9, 4, 6],\n",
    "              'max_features': ['log2', 'sqrt','auto'], \n",
    "              'criterion': ['entropy', 'gini'],\n",
    "              'max_depth': [10, 2, 3, 5], \n",
    "              'min_samples_split': [2, 3, 5],\n",
    "              'min_samples_leaf': [5, 1,8]\n",
    "             }\n",
    "\n",
    "# Type of scoring used to compare parameter combinations\n",
    "acc_scorer = make_scorer(accuracy_score)\n",
    "\n",
    "# Run the grid search\n",
    "grid_obj = GridSearchCV(ab, parameters, n_jobs=4)#, scoring=acc_scorer)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Set the clf to the best combination of parameters\n",
    "ab = grid_obj.best_estimator_\n",
    "\n",
    "\n",
    "ab.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, ab.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegressionCV()\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, logreg.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, clf.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pd.concat([X, y], axis=1).to_csv('perfect_numeric_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmodel = xgb.XGBClassifier()\n",
    "xmodel.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, xmodel.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
